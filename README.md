# ğŸ  SCRAPER NIERUCHOMOÅšCI - OTODOM.PL (MySQL)

Zaawansowany system scrapowania ogÅ‚oszeÅ„ mieszkaniowych z Otodom.pl z peÅ‚nym szczegÃ³Å‚owym parsowaniem kaÅ¼dego ogÅ‚oszenia i integracjÄ… z bazÄ… danych MySQL.

## âœ¨ Kluczowe FunkcjonalnoÅ›ci V2.0:

- **ğŸ¯ PeÅ‚ny Scraping Otodom.pl:** Automatyczne wchodzenie w kaÅ¼de ogÅ‚oszenie dla maksymalnego pobrania danych
- **ğŸ” Zaawansowane Parsowanie AdresÃ³w:** Automatyczne rozdzielanie na ulicÄ™, dzielnicÄ™, miasto i wojewÃ³dztwo
- **ğŸ“‹ ID OgÅ‚oszeÅ„:** Pobieranie unikalnych ID z sekcji opisu kaÅ¼dego ogÅ‚oszenia
- **ğŸ¢ Cechy NieruchomoÅ›ci:** Wykrywanie balkonu (63.5%), garaÅ¼u (41.9%), windy (37.8%), ogrodu (6.8%)
- **ğŸ—ï¸ SzczegÃ³Å‚y Budynku:** Rok budowy, piÄ™tro, typ budynku, standard wykoÅ„czenia, ogrzewanie
- **ğŸ”§ WyposaÅ¼enie AGD:** Zmywarka, lodÃ³wka, piekarnik i inne udogodnienia
- **ğŸ›¡ï¸ Zabezpieczenia:** Drzwi antywÅ‚amaniowe, domofon, monitoring
- **ğŸŒ Media:** Internet, TV kablowa, telefon
- **ğŸ—„ï¸ Kompletna Baza MySQL:** Z indeksami, constraintami, funkcjami i procedurami

## ğŸš€ SZYBKA INSTALACJA

### Opcja A - Automatyczna (zalecana):
```bash
python install.py
```

### Opcja B - Manualna:
```bash
# 1. Zainstaluj zaleÅ¼noÅ›ci
pip install -r requirements.txt

# 2. Skonfiguruj Å›rodowisko
cp .env_template .env
# Edytuj .env z danymi MySQL

# 3. UtwÃ³rz bazÄ™ danych
mysql -u root -p < sql/create_complete_database.sql

# 4. Przetestuj instalacjÄ™
python src/scrapers/otodom_scraper.py
```

## ğŸ“Š UÅ»YTKOWANIE

### Podstawowe komendy:
```bash
# Scraping 2 stron z peÅ‚nymi szczegÃ³Å‚ami (domyÅ›lnie)
python scripts/scraper_main.py --pages 2

# Szybki scraping bez szczegÃ³Å‚Ã³w
python scripts/scraper_main.py --pages 3 --no-details

# Tylko scraping bez geocodingu
python scripts/scraper_main.py --pages 5 --scraping-only
```

### ZarzÄ…dzanie bazÄ…:
```bash
# Statystyki bazy
mysql -u root -p nieruchomosci_db -e "CALL GetDatabaseStats();"

# SprawdÅº jakoÅ›Ä‡ danych
mysql -u root -p nieruchomosci_db -e "SELECT * FROM vw_nieruchomosci_stats LIMIT 10;"
```

## ğŸ¯ JAKOÅšÄ† DANYCH

**PRZED modyfikacjÄ… (tylko listing):**
- Balkon: 0/74 (0.0%) âŒ
- GaraÅ¼: 0/74 (0.0%) âŒ  
- OgrÃ³d: 0/74 (0.0%) âŒ
- Winda: 0/74 (0.0%) âŒ

**PO modyfikacji (szczegÃ³Å‚owy scraping):**
- âœ… **Balkon: 47/74 (63.5%)**
- âœ… **GaraÅ¼: 31/74 (41.9%)**
- âœ… **OgrÃ³d: 5/74 (6.8%)**
- âœ… **Winda: 28/74 (37.8%)**
- âœ… **ID ogÅ‚oszenia: 73/74 (98.6%)**
- âœ… **Ulica: 61/74 (82.4%)**
- âœ… **Miasto: 74/74 (100.0%)**
- âœ… **WojewÃ³dztwo: 74/74 (100.0%)**

## ğŸ¤– AUTOMATYZACJA - GITHUB ACTIONS

Scraper moÅ¼e byÄ‡ uruchamiany automatycznie co 6 godzin przy uÅ¼yciu GitHub Actions:

### ğŸ”§ Konfiguracja
1. **Skonfiguruj sekrety w GitHub**: `Settings â†’ Secrets and variables â†’ Actions`
   - `MYSQL_HOST`, `MYSQL_PORT`, `MYSQL_USER`, `MYSQL_PASSWORD`, `MYSQL_DATABASE`
   - `WEBHOOK_URL` (opcjonalne - dla powiadomieÅ„ Slack/Discord)

2. **Test lokalny** przed wÅ‚Ä…czeniem GitHub Actions:
   ```bash
   python scripts/test_github_actions.py
   ```

3. **Uruchomienie rÄ™czne**: ZakÅ‚adka `Actions` â†’ `ğŸ  Scraper NieruchomoÅ›ci` â†’ `Run workflow`

### ğŸ“Š FunkcjonalnoÅ›ci workflow
- âœ… Automatyczne uruchamianie co 6 godzin (00:00, 06:00, 12:00, 18:00 UTC)
- âœ… RÄ™czne uruchamianie z konfigurowalnymi parametrami
- âœ… Instalacja Chrome/ChromeDriver dla Selenium
- âœ… Test poÅ‚Ä…czenia z bazÄ… danych przed scrapingiem
- âœ… Powiadomienia o statusie (Slack/Discord)
- âœ… Artefakty z logami (przechowywane 7 dni)
- âœ… Statystyki koÅ„cowe i podsumowania

SzczegÃ³Å‚y konfiguracji: **[docs/github-actions-setup.md](docs/github-actions-setup.md)**

## ğŸ“š DOKUMENTACJA

- **[INSTRUKCJE_INSTALACJI.md](INSTRUKCJE_INSTALACJI.md)** - Kompletny przewodnik instalacji
- **[sql/create_complete_database.sql](sql/create_complete_database.sql)** - Skrypt bazy danych
- **[.env_template](.env_template)** - Szablon konfiguracji
- **[docs/github-actions-setup.md](docs/github-actions-setup.md)** - Konfiguracja GitHub Actions

## ğŸ“Š Podsumowanie Migracji

SzczegÃ³Å‚owe informacje na temat migracji z Supabase na MySQL, kluczowych zmian i zalet nowej konfiguracji znajdziesz w:

[MIGRACJA_MYSQL_PODSUMOWANIE.md](MIGRACJA_MYSQL_PODSUMOWANIE.md)

## ğŸ”§ Struktura Projektu

```
scraper/
â”œâ”€â”€ scripts/               # GÅ‚Ã³wne skrypty do uruchamiania pipeline
â”œâ”€â”€ src/                   # Kod ÅºrÃ³dÅ‚owy (scrapery, parsery, geocoding)
â”‚   â”œâ”€â”€ scrapers/          # ModuÅ‚y do scrapowania danych z portali
â”‚   â”œâ”€â”€ parsers/           # ModuÅ‚y do parsowania adresÃ³w
â”‚   â””â”€â”€ geocoding/         # ModuÅ‚y do geocodingu
â”œâ”€â”€ sql/                   # Skrypty SQL do tworzenia/zarzÄ…dzania bazÄ… MySQL
â”œâ”€â”€ tests/                 # Testy jednostkowe i integracyjne
â”œâ”€â”€ docs/                  # Dodatkowa dokumentacja
â”œâ”€â”€ legacy/                # Starsze, nieuÅ¼ywane juÅ¼ wersje kodu/plikÃ³w
â”œâ”€â”€ mysql_utils.py         # ModuÅ‚ do obsÅ‚ugi bazy danych MySQL
â”œâ”€â”€ env_example.txt        # PrzykÅ‚adowy plik konfiguracyjny .env
â”œâ”€â”€ requirements.txt       # Lista zaleÅ¼noÅ›ci Python
â”œâ”€â”€ INSTRUKCJE_URUCHOMIENIA_MYSQL.md # **GÅ‚Ã³wna dokumentacja uruchomienia**
â”œâ”€â”€ MIGRACJA_MYSQL_PODSUMOWANIE.md # **Podsumowanie migracji z Supabase na MySQL**
â””â”€â”€ README.md              # Ten plik
```

## ğŸ”§ Najnowsze poprawki geocodingu

### Problem z fuzzywuzzy

**BÅ‚Ä…d:** Scraper wyrzucaÅ‚ bÅ‚Ä…d `ImportError: Biblioteka fuzzywuzzy nie jest zainstalowana` i przerywaÅ‚ dziaÅ‚anie.

**RozwiÄ…zanie:**
1. âœ… **Poprawiono deduplicator** (`src/deduplication/deduplicator.py`):
   - Dodano alternatywne funkcje porÃ³wnania tekstÃ³w gdy `fuzzywuzzy` nie jest dostÄ™pne
   - Zaimplementowano `simple_ratio()` i `levenshtein_ratio()` jako zamienniki
   - Deduplicator teraz dziaÅ‚a niezaleÅ¼nie od dostÄ™pnoÅ›ci biblioteki

2. âœ… **Poprawiono instalacjÄ™ w GitHub Actions**:
   - Dodano jawnÄ… instalacjÄ™ `fuzzywuzzy==0.18.0` i `python-Levenshtein==0.23.0`
   - Dodano testy importÃ³w dla bibliotek tekstowych

### Problem z optimized geocoder

**BÅ‚Ä…d:** Optimized geocoder uÅ¼ywaÅ‚ nieistniejÄ…cego `supabase_utils.py`.

**RozwiÄ…zanie:**
1. âœ… **Przepisano optimized geocoder** na MySQL:
   - Zmieniono importy z `supabase_utils` na `mysql_utils`
   - Przepisano funkcje bazy danych na SQL zamiast Supabase API
   - Poprawiono nazwy kolumn (`street` zamiast `street_name`)

2. âœ… **Stworzono niezawodny geocoder**:
   - Dodano funkcjÄ™ `main_geocoding_process()` w `src/geocoding/geocoder.py`
   - Prosty, stabilny proces geocodingu bez async/await
   - Lepsze raportowanie bÅ‚Ä™dÃ³w i statystyk

### Aktualne dziaÅ‚anie geocodingu

**Scraper gÅ‚Ã³wny:** UÅ¼ywa `main_geocoding_process()` - prosty i niezawodny
**Test geocodera:** `python src/geocoding/geocoder.py --test`
**RÄ™czny geocoding:** `python src/geocoding/geocoder.py --run --max-addresses 100`

### Biblioteki wymagane dla geocodingu
- `requests` - zapytania HTTP do Nominatim OSM
- `mysql-connector-python` - poÅ‚Ä…czenie z bazÄ… MySQL  
- `fuzzywuzzy` + `python-Levenshtein` - porÃ³wnanie tekstÃ³w (opcjonalne)

Wszystkie biblioteki sÄ… automatycznie instalowane w GitHub Actions.

---