# ğŸš€ GitHub Actions - Konfiguracja Scrapera

## ğŸ“‹ PrzeglÄ…d

Ten projekt zawiera automatyczny workflow GitHub Actions do uruchamiania scrapera nieruchomoÅ›ci w regularnych odstÄ™pach czasu.

## âš™ï¸ Konfiguracja Repository Secrets

Aby GitHub Actions dziaÅ‚aÅ‚y poprawnie, musisz skonfigurowaÄ‡ nastÄ™pujÄ…ce sekrety w swoim repozytorium:

### ğŸ” Wymagane sekrety MySQL
IdÅº do `Settings > Secrets and variables > Actions` i dodaj:

- `MYSQL_HOST` - adres serwera MySQL (np. `s108.cyber-folks.pl`)
- `MYSQL_PORT` - port MySQL (np. `3306`)  
- `MYSQL_USER` - nazwa uÅ¼ytkownika MySQL
- `MYSQL_PASSWORD` - hasÅ‚o do bazy MySQL
- `MYSQL_DATABASE` - nazwa bazy danych

### ğŸ“¢ Opcjonalne powiadomienia
- `WEBHOOK_URL` - URL webhook'a Slack/Discord do powiadomieÅ„ (opcjonalne)
  - JeÅ›li nie ustawione, powiadomienia bÄ™dÄ… pomijane

## ğŸ•’ Harmonogram uruchamiania

### Automatyczne uruchamianie
Workflow uruchamia siÄ™ automatycznie **co 6 godzin**:
- 00:00 UTC
- 06:00 UTC  
- 12:00 UTC
- 18:00 UTC

### RÄ™czne uruchamianie
MoÅ¼esz uruchomiÄ‡ scraper rÄ™cznie z nastÄ™pujÄ…cymi parametrami:
- **max_pages**: Maksymalna liczba stron (domyÅ›lnie: 5)
- **batch_size**: Rozmiar batcha zapisu (domyÅ›lnie: 100)
- **scraping_only**: Tylko scrapowanie bez geocodingu (domyÅ›lnie: false)
- **no_details**: PomiÅ„ szczegÃ³Å‚owy scraping (domyÅ›lnie: false)

## ğŸ”§ Co robi workflow

1. **ğŸ Przygotowanie Å›rodowiska**
   - Instaluje Python 3.11
   - Instaluje Chrome i ChromeDriver dla Selenium
   - Instaluje wszystkie zaleÅ¼noÅ›ci z `requirements.txt`

2. **ğŸ”§ Konfiguracja**
   - Tworzy plik `.env` z sekretami
   - Testuje poÅ‚Ä…czenie z bazÄ… MySQL

3. **ğŸ” Scraping**
   - Uruchamia scraper z odpowiednimi parametrami
   - Zapisuje dane do bazy MySQL
   - Wykonuje geocoding (jeÅ›li wÅ‚Ä…czony)

4. **ğŸ“Š Raportowanie**
   - WyÅ›wietla statystyki koÅ„cowe
   - Zapisuje logi jako artefakty
   - WysyÅ‚a powiadomienia o statusie (jeÅ›li skonfigurowane)

## ğŸ“ Artefakty

Po kaÅ¼dym uruchomieniu workflow zapisuje:
- Logi scrapera (`scraper-logs-{run_number}`)
- Przechowywane przez 7 dni

## ğŸš¨ Powiadomienia

JeÅ›li skonfigurujesz `WEBHOOK_URL`, otrzymasz powiadomienia:

### âœ… Sukces
```
âœ… Scraper nieruchomoÅ›ci: ZakoÅ„czony pomyÅ›lnie
ğŸ“Š ÅÄ…cznie: 1,234 ogÅ‚oszeÅ„
ğŸ†• Dodane dzisiaj: 45
â° Ostatnia godzina: 12
ğŸ’° Z cenami: 1,200
ğŸŒ Geocoded: 980
ğŸ”— SzczegÃ³Å‚y: [link do workflow]
```

### âŒ BÅ‚Ä…d
```
âŒ Scraper nieruchomoÅ›ci: BÅ‚Ä…d podczas wykonania
ğŸ”— Link: [link do workflow]
```

## ğŸ§ª Testowanie przed uruchomieniem

### Test lokalny
```bash
python scripts/test_github_actions.py
```

### Test poÅ‚Ä…czenia w GitHub
1. IdÅº do zakÅ‚adki `Actions` â†’ `ğŸ§ª Test PoÅ‚Ä…czenia z BazÄ…`
2. Kliknij `Run workflow`
3. Ten test zajmuje 2-3 minuty i sprawdza tylko poÅ‚Ä…czenie z bazÄ…

## ğŸ” Monitorowanie

### Sprawdzanie statusu workflow
1. IdÅº do zakÅ‚adki `Actions` w swoim repozytorium
2. Wybierz workflow `ğŸ  Scraper NieruchomoÅ›ci`
3. Zobacz historiÄ™ uruchomieÅ„ i ich statusy

### Pobieranie logÃ³w
1. WejdÅº w konkretne uruchomienie workflow
2. Pobierz artefakt `scraper-logs-{number}`
3. Rozpakuj i przejrzyj logi

## âš¡ PrzykÅ‚ady uruchomienia

### Szybki test (5 stron, bez geocodingu)
```yaml
max_pages: "5"
scraping_only: true
```

### PeÅ‚ny scraping (20 stron z geocodingiem)  
```yaml
max_pages: "20"
batch_size: "50"
scraping_only: false
```

### Tylko lista bez szczegÃ³Å‚Ã³w
```yaml
max_pages: "10"
no_details: true
batch_size: "200"
```

## ğŸ› ï¸ RozwiÄ…zywanie problemÃ³w

### BÅ‚Ä…d poÅ‚Ä…czenia z bazÄ…
- SprawdÅº czy sekrety MySQL sÄ… poprawnie skonfigurowane
- Zweryfikuj czy serwer MySQL jest dostÄ™pny

### Timeout workflow
- Workflow ma limit 60 minut
- Zmniejsz `max_pages` lub zwiÄ™ksz `batch_size`

### BÅ‚Ä™dy Selenium
- Chrome i ChromeDriver sÄ… automatycznie instalowane
- Scraper uÅ¼ywa trybu headless (bez GUI)

### Brak powiadomieÅ„
- SprawdÅº czy `WEBHOOK_URL` jest poprawnie skonfigurowany
- Zweryfikuj format webhook'a (Slack/Discord)

## ğŸ”„ Modyfikacja harmonogramu

Aby zmieniÄ‡ czÄ™stotliwoÅ›Ä‡ uruchamiania, edytuj `.github/workflows/scraper.yml`:

```yaml
schedule:
  - cron: '0 */6 * * *'  # Co 6 godzin
  # - cron: '0 8 * * *'   # Codziennie o 8:00 UTC  
  # - cron: '0 */3 * * *' # Co 3 godziny
```

## ğŸ“ˆ Optymalizacja wydajnoÅ›ci

### Dla szybkiego scrapingu
```yaml
max_pages: "3"
batch_size: "50"
no_details: true
```

### Dla dokÅ‚adnych danych
```yaml
max_pages: "10" 
batch_size: "25"
scraping_only: false
```

## ğŸ”’ BezpieczeÅ„stwo

- Wszystkie dane dostÄ™powe sÄ… przechowywane jako GitHub Secrets
- Workflow wykorzystuje najnowsze wersje akcji
- Logi nie zawierajÄ… wraÅ¼liwych danych
- Automatic cleanup artefaktÃ³w po 7 dniach 