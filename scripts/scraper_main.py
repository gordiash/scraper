#!/usr/bin/env python3
"""
GÅÃ“WNY SKRIPT SCRAPERA - NOWA STRUKTURA BAZY DANYCH MYSQL
ÅÄ…czy scraping â†’ parsing adresÃ³w â†’ geocoding w jeden proces
Zaktualizowany do nowej struktury z polami market, has_balcony, itp. + MySQL
"""
import logging
import sys
import os
import argparse
from datetime import datetime
from typing import List, Dict

# Dodaj gÅ‚Ã³wny katalog do Å›cieÅ¼ki
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.scrapers.otodom_scraper import get_otodom_listings, DEFAULT_BASE_URL
from src.parsers.address_parser import process_all_locations
from src.geocoding.geocoder import update_all_coordinates_improved
from mysql_utils import save_listings_to_mysql, get_mysql_connection
from src.deduplication.deduplicator import deduplicate_listings, generate_duplicate_report

# Konfiguracja logowania
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('scraper.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

def print_banner():
    """WyÅ›wietl banner aplikacji"""
    print("="*80)
    print("ğŸ  SCRAPER NIERUCHOMOÅšCI - MYSQL + NOWA STRUKTURA BAZY DANYCH")
    print("="*80)
    print("ğŸ“… Data uruchomienia:", datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("ğŸ”— Å¹rÃ³dÅ‚o: Otodom.pl")
    print("ğŸ’¾ Baza danych: MySQL (nowa struktura)")
    print("ğŸŒ Geocoding: OpenStreetMap Nominatim")
    print("ğŸ·ï¸ Cechy: market, has_balcony, has_garage, has_garden, has_elevator")
    print("="*80)

def get_database_stats() -> Dict[str, int]:
    """Pobierz statystyki z bazy danych MySQL"""
    try:
        connection = get_mysql_connection()
        cursor = connection.cursor()
        
        # Statystyki nieruchomosci
        cursor.execute("SELECT COUNT(*) FROM nieruchomosci")
        total_listings = cursor.fetchone()[0]
        
        # Statystyki z cenami
        cursor.execute("SELECT COUNT(*) FROM nieruchomosci WHERE price IS NOT NULL")
        with_price_count = cursor.fetchone()[0]
        
        # Statystyki z powierzchniÄ…
        cursor.execute("SELECT COUNT(*) FROM nieruchomosci WHERE area IS NOT NULL")
        with_area_count = cursor.fetchone()[0]
        
        # Statystyki geocoded
        cursor.execute("SELECT COUNT(*) FROM nieruchomosci WHERE latitude IS NOT NULL")
        geocoded_count = cursor.fetchone()[0]
        
        # Statystyki rynku
        cursor.execute("SELECT COUNT(*) FROM nieruchomosci WHERE market = 'pierwotny'")
        primary_market_count = cursor.fetchone()[0]
        
        cursor.execute("SELECT COUNT(*) FROM nieruchomosci WHERE market = 'wtÃ³rny'")
        secondary_market_count = cursor.fetchone()[0]
        
        # Statystyki udogodnieÅ„
        cursor.execute("SELECT COUNT(*) FROM nieruchomosci WHERE has_balcony = 1")
        with_balcony_count = cursor.fetchone()[0]
        
        cursor.execute("SELECT COUNT(*) FROM nieruchomosci WHERE has_garage = 1")
        with_garage_count = cursor.fetchone()[0]
        
        cursor.close()
        connection.close()
        
        return {
            "total_listings": total_listings,
            "with_price_count": with_price_count,
            "with_area_count": with_area_count,
            "geocoded_count": geocoded_count,
            "primary_market_count": primary_market_count,
            "secondary_market_count": secondary_market_count,
            "with_balcony_count": with_balcony_count,
            "with_garage_count": with_garage_count
        }
        
    except Exception as e:
        logger.error(f"âŒ BÅ‚Ä…d pobierania statystyk z MySQL: {e}")
        return {
            "total_listings": 0,
            "with_price_count": 0,
            "with_area_count": 0,
            "geocoded_count": 0,
            "primary_market_count": 0,
            "secondary_market_count": 0,
            "with_balcony_count": 0,
            "with_garage_count": 0
        }

def print_stats(title: str, stats: Dict[str, int]):
    """WyÅ›wietl statystyki"""
    print(f"\nğŸ“Š {title}")
    print("-" * 60)
    print(f"ğŸ“‹ ÅÄ…cznie ogÅ‚oszeÅ„: {stats['total_listings']:,}")
    print(f"ğŸ’° Z cenami: {stats['with_price_count']:,}")
    print(f"ğŸ“ Z powierzchniÄ…: {stats['with_area_count']:,}")
    print(f"ğŸŒ Z wspÃ³Å‚rzÄ™dnymi: {stats['geocoded_count']:,}")
    
    if stats['total_listings'] > 0:
        price_rate = (stats['with_price_count'] / stats['total_listings']) * 100
        area_rate = (stats['with_area_count'] / stats['total_listings']) * 100
        geocoding_rate = (stats['geocoded_count'] / stats['total_listings']) * 100
        print(f"ğŸ“ˆ Pokrycie cen: {price_rate:.1f}%")
        print(f"ğŸ“ˆ Pokrycie powierzchni: {area_rate:.1f}%")
        print(f"ğŸ“ˆ Pokrycie geocoding: {geocoding_rate:.1f}%")
    
    print(f"\nğŸ·ï¸ STATYSTYKI RYNKU:")
    print(f"ğŸ†• Rynek pierwotny: {stats['primary_market_count']:,}")
    print(f"ğŸ˜ï¸ Rynek wtÃ³rny: {stats['secondary_market_count']:,}")
    
    print(f"\nğŸ  STATYSTYKI UDOGODNIEÅƒ:")
    print(f"ğŸ¢ Z balkonem: {stats['with_balcony_count']:,}")
    print(f"ğŸš— Z garaÅ¼em: {stats['with_garage_count']:,}")

def run_scraping_phase(max_pages: int, scrape_details: bool = True, base_url: str = DEFAULT_BASE_URL, batch_size: int = 0, enable_scraper_geocoding: bool = True) -> List[Dict]:
    """
    Faza 1: Scrapowanie ogÅ‚oszeÅ„ z Otodom.pl z nowÄ… strukturÄ… danych
    
    Args:
        max_pages: Maksymalna liczba stron do scrapowania
        scrape_details: Czy pobieraÄ‡ szczegÃ³Å‚owe dane z indywidualnych stron
    
    Returns:
        List[Dict]: Lista pobranych ogÅ‚oszeÅ„
    """
    print(f"\nğŸ” FAZA 1: SCRAPOWANIE OTODOM.PL {'+ SZCZEGÃ“ÅOWE DANE' if scrape_details else '(TYLKO LISTA)'}")
    print(f"ğŸ“„ Maksymalna liczba stron: {'WSZYSTKIE' if (max_pages is None or max_pages <= 0) else max_pages}")
    print(f"ğŸ” SzczegÃ³Å‚owy scraping: {'âœ… TAK' if scrape_details else 'âŒ NIE'}")
    print(f"ğŸ’¾ Batch zapis: {'co ' + str(batch_size) + ' ofert' if batch_size else 'po zakoÅ„czeniu'}")
    print("-" * 60)
    
    try:
        # Funkcja zapisu batcha
        def batch_save(batch: List[Dict]):
            if not batch:
                return
            print(f"\nğŸ’¾ Zapis batcha ({len(batch)}) do bazyâ€¦")
            unique_batch = deduplicate_listings(batch, similarity_threshold=75.0, keep_best_source=True)
            saved = save_listings_to_mysql(unique_batch, require_complete=False)
            print(f"âœ… Batch zapisany: {saved}/{len(unique_batch)} rekordÃ³w")

        listings = get_otodom_listings(base_url=base_url,
                                       max_pages=max_pages,
                                       scrape_details=scrape_details,
                                       batch_size=batch_size,
                                       batch_callback=batch_save if batch_size else None,
                                       resume=False,
                                       enable_geocoding=enable_scraper_geocoding)
        
        if listings:
            print(f"âœ… Pobrano {len(listings)} ogÅ‚oszeÅ„ z Otodom.pl")
            
            # Statystyki jakoÅ›ci danych
            with_price = len([l for l in listings if l.get('price')])
            with_address = len([l for l in listings if l.get('address_raw')])
            with_area = len([l for l in listings if l.get('area')])
            with_rooms = len([l for l in listings if l.get('rooms')])
            with_city = len([l for l in listings if l.get('city')])
            
            # Statystyki nowych pÃ³l
            with_balcony = len([l for l in listings if l.get('has_balcony')])
            with_garage = len([l for l in listings if l.get('has_garage')])
            with_garden = len([l for l in listings if l.get('has_garden')])
            with_elevator = len([l for l in listings if l.get('has_elevator')])
            
            primary_market = len([l for l in listings if l.get('market') == 'pierwotny'])
            secondary_market = len([l for l in listings if l.get('market') == 'wtÃ³rny'])
            
            print(f"ğŸ’° Z cenami: {with_price}/{len(listings)} ({with_price/len(listings)*100:.1f}%)")
            print(f"ğŸ“ Z adresami: {with_address}/{len(listings)} ({with_address/len(listings)*100:.1f}%)")
            print(f"ğŸ“ Z powierzchniÄ…: {with_area}/{len(listings)} ({with_area/len(listings)*100:.1f}%)")
            print(f"ğŸšª Z pokojami: {with_rooms}/{len(listings)} ({with_rooms/len(listings)*100:.1f}%)")
            print(f"ğŸ™ï¸ Z miastem: {with_city}/{len(listings)} ({with_city/len(listings)*100:.1f}%)")
            
            print(f"\nğŸ·ï¸ STATYSTYKI RYNKU:")
            print(f"ğŸ†• Rynek pierwotny: {primary_market}/{len(listings)} ({primary_market/len(listings)*100:.1f}%)")
            print(f"ğŸ˜ï¸ Rynek wtÃ³rny: {secondary_market}/{len(listings)} ({secondary_market/len(listings)*100:.1f}%)")
            
            print(f"\nğŸ  STATYSTYKI UDOGODNIEÅƒ:")
            print(f"ğŸ¢ Z balkonem: {with_balcony}/{len(listings)} ({with_balcony/len(listings)*100:.1f}%)")
            print(f"ğŸš— Z garaÅ¼em: {with_garage}/{len(listings)} ({with_garage/len(listings)*100:.1f}%)")
            print(f"ğŸŒ¿ Z ogrodem: {with_garden}/{len(listings)} ({with_garden/len(listings)*100:.1f}%)")
            print(f"ğŸ›— Z windÄ…: {with_elevator}/{len(listings)} ({with_elevator/len(listings)*100:.1f}%)")
            
            # Dodatkowe statystyki szczegÃ³Å‚owych danych (jeÅ›li dostÄ™pne)
            if scrape_details:
                with_year = len([l for l in listings if l.get('year_of_construction')])
                with_floor = len([l for l in listings if l.get('floor')])
                with_building_type = len([l for l in listings if l.get('building_type')])
                with_finish = len([l for l in listings if l.get('standard_of_finish')])
                
                print(f"\nğŸ—ï¸ STATYSTYKI SZCZEGÃ“ÅOWYCH DANYCH:")
                print(f"ğŸ“… Z rokiem budowy: {with_year}/{len(listings)} ({with_year/len(listings)*100:.1f}%)")
                print(f"ğŸ¢ Z piÄ™trem: {with_floor}/{len(listings)} ({with_floor/len(listings)*100:.1f}%)")
                print(f"ğŸ˜ï¸ Z typem budynku: {with_building_type}/{len(listings)} ({with_building_type/len(listings)*100:.1f}%)")
                print(f"ğŸ¨ Ze stanem wykoÅ„czenia: {with_finish}/{len(listings)} ({with_finish/len(listings)*100:.1f}%)")
            
            return listings
        else:
            print("âŒ Nie pobrano Å¼adnych ogÅ‚oszeÅ„")
            return []
            
    except Exception as e:
        logger.error(f"âŒ BÅ‚Ä…d w fazie scrapowania: {e}")
        return []

def run_saving_phase(listings: List[Dict]) -> int:
    """
    Faza 2: Zapis ogÅ‚oszeÅ„ do bazy danych MySQL (nowa struktura)
    
    Args:
        listings: Lista ogÅ‚oszeÅ„ do zapisu
    
    Returns:
        int: Liczba zapisanych ogÅ‚oszeÅ„
    """
    print(f"\nğŸ’¾ FAZA 2: ZAPIS DO BAZY DANYCH MYSQL (NOWA STRUKTURA)")
    print(f"ğŸ“‹ OgÅ‚oszeÅ„ do zapisu: {len(listings)}")
    print("-" * 60)
    
    try:
        saved_count = save_listings_to_mysql(listings, require_complete=False)
        
        if saved_count > 0:
            print(f"âœ… Zapisano {saved_count} nowych ogÅ‚oszeÅ„ do MySQL")
            
            # Statystyki zapisu
            duplicate_count = len(listings) - saved_count
            if duplicate_count > 0:
                print(f"â­ï¸ PominiÄ™to {duplicate_count} duplikatÃ³w")
                
            success_rate = (saved_count / len(listings)) * 100
            print(f"ğŸ“ˆ SkutecznoÅ›Ä‡ zapisu: {success_rate:.1f}%")
        else:
            print("âš ï¸ Nie zapisano Å¼adnych nowych ogÅ‚oszeÅ„ (wszystkie to duplikaty)")
            
        return saved_count
        
    except Exception as e:
        logger.error(f"âŒ BÅ‚Ä…d w fazie zapisu do MySQL: {e}")
        return 0

def run_geocoding_phase(max_addresses: int) -> bool:
    """
    Faza 3: Geocoding - uzupeÅ‚nianie wspÃ³Å‚rzÄ™dnych
    
    Args:
        max_addresses: Maksymalna liczba adresÃ³w do geocodingu
    
    Returns:
        bool: True jeÅ›li geocoding siÄ™ udaÅ‚
    """
    print(f"\nğŸŒ FAZA 3: GEOCODING WSPÃ“ÅRZÄ˜DNYCH")
    print(f"ğŸ“ Maksymalna liczba adresÃ³w: {max_addresses}")
    print("-" * 60)
    
    try:
        # Zoptymalizowane parametry
        optimal_batch_size = min(100, max_addresses) if max_addresses else 100
        
        print(f"âš¡ Parametry optymalizacji:")
        print(f"   â€¢ Batch size: {optimal_batch_size}")
        print(f"   â€¢ OpÃ³Åºnienie: 1.0s miÄ™dzy requestami")
        print(f"   â€¢ Max retries: 2")
        
        # Uruchom geocoding (funkcja wyÅ›wietla wÅ‚asne statystyki)
        update_all_coordinates_improved(max_addresses=max_addresses)
        print("âœ… Geocoding zakoÅ„czony")
        return True
        
    except Exception as e:
        logger.error(f"âŒ BÅ‚Ä…d w fazie geocodingu: {e}")
        return False

def run_complete_pipeline(max_pages: int = 0, max_geocoding_addresses: int = 100, scrape_details: bool = True, base_url: str = DEFAULT_BASE_URL, batch_size: int = 0, enable_scraper_geocoding: bool = True) -> bool:
    """
    Uruchamia kompletny pipeline: scraping â†’ zapis â†’ geocoding
    
    Args:
        max_pages: Maksymalna liczba stron do scrapowania
        max_geocoding_addresses: Maksymalna liczba adresÃ³w do geocodingu
        scrape_details: Czy pobieraÄ‡ szczegÃ³Å‚owe dane z indywidualnych stron
    
    Returns:
        bool: True jeÅ›li wszystkie fazy siÄ™ udaÅ‚y
    """
    print_banner()
    
    # Statystyki poczÄ…tkowe
    initial_stats = get_database_stats()
    print_stats("STATYSTYKI POCZÄ„TKOWE", initial_stats)
    
    # FAZA 1: Scrapowanie
    listings = run_scraping_phase(max_pages, scrape_details, base_url=base_url, batch_size=batch_size, enable_scraper_geocoding=enable_scraper_geocoding)
    if not listings:
        print("âŒ Brak danych do dalszego przetwarzania")
        return False

    # FAZA 2: Deduplikacja
    print(f"\nâœ¨ FAZA 2: DEDUPLIKACJA OGÅOSZEÅƒ")
    print(f"ğŸ“‹ OgÅ‚oszeÅ„ przed deduplikacjÄ…: {len(listings)}")
    print("-" * 60)
    
    unique_listings = deduplicate_listings(listings, similarity_threshold=75.0, keep_best_source=True)
    duplicates_found = len(listings) - len(unique_listings)
    
    print(f"âœ… OgÅ‚oszeÅ„ po deduplikacji: {len(unique_listings)}")
    print(f"â­ï¸ UsuniÄ™to duplikatÃ³w: {duplicates_found}")
    
    if duplicates_found > 0:
        # Opcjonalnie moÅ¼na by wygenerowaÄ‡ raport tutaj, jeÅ›li potrzebny jest szczegÃ³Å‚owy log
        pass # generate_duplicate_report(duplicates_list) jeÅ›li lista duplikatÃ³w jest zwracana

    # FAZA 3: Zapis do bazy
    saved_count = run_saving_phase(unique_listings)
    
    # FAZA 4: Geocoding (tylko jeÅ›li zapisano nowe dane)
    geocoding_success = True
    if saved_count > 0:
        geocoding_success = run_geocoding_phase(max_geocoding_addresses)
    else:
        print(f"\nğŸŒ FAZA 3: GEOCODING POMINIÄ˜TY")
        print("ğŸ’¡ Brak nowych danych do geocodingu")
    
    # Statystyki koÅ„cowe
    final_stats = get_database_stats()
    print_stats("STATYSTYKI KOÅƒCOWE", final_stats)
    
    # Podsumowanie
    print(f"\nğŸ‰ PODSUMOWANIE PIPELINE:")
    print("="*60)
    print(f"ğŸ“Š Pobrano ogÅ‚oszeÅ„: {len(listings)}")
    print(f"ğŸ’¾ Zapisano nowych: {saved_count}")
    print(f"ğŸŒ Geocoding: {'âœ… OK' if geocoding_success else 'âŒ BÅÄ„D'}")
    
    # Przyrost danych
    new_listings = final_stats['total_listings'] - initial_stats['total_listings']
    if new_listings > 0:
        print(f"ğŸ“ˆ Przyrost w bazie: +{new_listings} ogÅ‚oszeÅ„")
    
    return True

def main():
    """GÅ‚Ã³wna funkcja"""
    parser = argparse.ArgumentParser(description='Scraper nieruchomoÅ›ci - MySQL + nowa struktura bazy')
    parser.add_argument('--pages', type=int, default=0, help='Maksymalna liczba stron do scrapowania (0 = wszystkie)')
    parser.add_argument('--geocoding', type=int, default=100, help='Maksymalna liczba adresÃ³w do geocodingu')
    parser.add_argument('--scraping-only', action='store_true', help='Tylko scrapowanie bez geocodingu')
    parser.add_argument('--no-details', action='store_true', help='PomiÅ„ szczegÃ³Å‚owy scraping (tylko lista)')
    parser.add_argument('--no-scraper-geocoding', action='store_true', help='WyÅ‚Ä…cz geocoding w scrapperze (uÅ¼yj osobny proces)')
    parser.add_argument('--url', type=str, help='Niestandardowy URL wynikÃ³w Otodom (opcjonalnie)')
    parser.add_argument('--batch-size', type=int, default=100, help='Rozmiar batcha do zapisu (0 = zapis na koÅ„cu)')
    
    args = parser.parse_args()
    
    # OkreÅ›l czy scraping szczegÃ³Å‚Ã³w
    scrape_details = not args.no_details
    
    # OkreÅ›l czy geocoding w scrapperze
    enable_scraper_geocoding = not args.no_scraper_geocoding
    
    try:
        if args.scraping_only:
            # Tylko scrapowanie i zapis
            print_banner()
            initial_stats = get_database_stats()
            print_stats("STATYSTYKI POCZÄ„TKOWE", initial_stats)
            
            base_url = args.url or DEFAULT_BASE_URL
            batch_size = args.batch_size
            listings = run_scraping_phase(args.pages, scrape_details, base_url=base_url, batch_size=batch_size)
            if listings:
                saved_count = run_saving_phase(listings)
                print(f"\nğŸ‰ ZAKOÅƒCZONO: Pobrano {len(listings)}, zapisano {saved_count}")
            else:
                print("âŒ Nie pobrano Å¼adnych danych")
        else:
            # Kompletny pipeline
            success = run_complete_pipeline(args.pages, args.geocoding, scrape_details, args.url or DEFAULT_BASE_URL, batch_size=args.batch_size, enable_scraper_geocoding=enable_scraper_geocoding)
            if success:
                print(f"\nğŸ‰ PIPELINE ZAKOÅƒCZONY POMYÅšLNIE!")
            else:
                print(f"\nâŒ PIPELINE ZAKOÅƒCZONY Z BÅÄ˜DAMI")
                
    except KeyboardInterrupt:
        print(f"\nâš ï¸ Przerwano przez uÅ¼ytkownika")
    except Exception as e:
        logger.error(f"âŒ BÅ‚Ä…d gÅ‚Ã³wnego procesu: {e}")
        print(f"\nâŒ BÅ‚Ä…d: {e}")

if __name__ == "__main__":
    main() 