#!/usr/bin/env python3
"""
SCRAPER OTODOM.PL
Scraper dla portalu Otodom.pl z pe≈ÇnƒÖ obs≈ÇugƒÖ Selenium
Zaktualizowany do nowej struktury bazy danych
"""
import logging
import sys
import os
import re
from typing import List, Dict, Optional, Callable
import json
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import time
import random

# Dodaj g≈Ç√≥wny katalog do ≈õcie≈ºki
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from utils import get_soup, random_delay, clean_text, extract_price

# Import geocodingu 
try:
    from src.geocoding.geocoder import geocode_address_improved, build_simple_search_query
    GEOCODING_AVAILABLE = True
except ImportError as e:
    logger.warning(f"‚ö†Ô∏è Geocoding niedostƒôpny: {e}")
    GEOCODING_AVAILABLE = False

# Konfiguracja logowania z obs≈ÇugƒÖ wielowƒÖtkowo≈õci
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s - [%(threadName)s] - %(message)s'
)
logger = logging.getLogger(__name__)

# Thread-safe licznik dla postƒôpu
_progress_lock = threading.Lock()
_progress_counter = {"current": 0, "total": 0}

DEFAULT_BASE_URL = "https://www.otodom.pl/pl/wyniki/sprzedaz/mieszkanie/cala-polska"

def scrape_listing_details_thread_safe(listing_data: Dict, enable_geocoding: bool = False) -> Dict:
    """
    Thread-safe wrapper dla scrapowania szczeg√≥≈Ç√≥w pojedynczego og≈Çoszenia
    
    Args:
        listing_data: Podstawowe dane og≈Çoszenia z URL
        enable_geocoding: Czy pobieraƒá wsp√≥≈Çrzƒôdne geograficzne
    
    Returns:
        Dict: Og≈Çoszenie z pobranymi szczeg√≥≈Çami
    """
    try:
        # Dodaj random delay ≈ºeby nie przeciƒÖ≈ºaƒá serwera
        time.sleep(random.uniform(0.5, 2.0))
        
        url = listing_data.get("url")
        if not url:
            return listing_data
            
        # Pobierz szczeg√≥≈Çy
        detailed_data = scrape_individual_listing(url)
        
        # Po≈ÇƒÖcz z podstawowymi danymi
        if detailed_data:
            listing_data.update(detailed_data)
        
        # GEOCODING: Pobierz wsp√≥≈Çrzƒôdne je≈õli w≈ÇƒÖczone
        if enable_geocoding and GEOCODING_AVAILABLE:
            try:
                # Zbuduj zapytanie geocodingu z dostƒôpnych danych
                address_data = {
                    'city': listing_data.get('city'),
                    'district': listing_data.get('district'), 
                    'street_name': listing_data.get('street'),
                    'address_raw': listing_data.get('address_raw')
                }
                
                geocoding_query = build_simple_search_query(address_data)
                
                if geocoding_query and geocoding_query != "Polska":
                    coordinates = geocode_address_improved(geocoding_query)
                    if coordinates:
                        latitude, longitude = coordinates
                        listing_data['latitude'] = latitude
                        listing_data['longitude'] = longitude
                        logger.debug(f"‚úÖ Geocoding: {latitude:.6f}, {longitude:.6f}")
                    else:
                        logger.debug(f"‚ö†Ô∏è Brak wsp√≥≈Çrzƒôdnych dla: {geocoding_query}")
                        
                # Dodaj op√≥≈∫nienie dla geocodingu
                time.sleep(1.0)
                        
            except Exception as e:
                logger.error(f"‚ùå B≈ÇƒÖd geocodingu: {e}")
            
        # Thread-safe update licznika postƒôpu
        with _progress_lock:
            _progress_counter["current"] += 1
            current = _progress_counter["current"]
            total = _progress_counter["total"]
            if current % 5 == 0 or current == total:  # Log co 5 og≈Çosze≈Ñ lub na ko≈Ñcu
                logger.info(f"üîç Postƒôp szczeg√≥≈Ç√≥w: {current}/{total} ({current/total*100:.1f}%)")
        
        return listing_data
        
    except Exception as e:
        logger.error(f"‚ùå B≈ÇƒÖd w wƒÖtku dla {listing_data.get('url', 'unknown')}: {e}")
        return listing_data

def extract_numeric_value(text: str) -> float:
    """Wydobywa warto≈õƒá numerycznƒÖ z tekstu"""
    if not text:
        return None
    
    # Znajd≈∫ pierwszƒÖ liczbƒô w tek≈õcie
    match = re.search(r'(\d+(?:[,.])\d+|\d+)', text.replace(' ', ''))
    if match:
        value_str = match.group(1).replace(',', '.')
        try:
            return float(value_str)
        except ValueError:
            pass
    return None

def extract_boolean_features(text: str) -> Dict[str, bool]:
    """Wydobywa cechy boolean z tekstu (balkon, gara≈º, ogr√≥d, winda)"""
    text_lower = text.lower()
    
    return {
        "has_balcony": any(word in text_lower for word in ['balkon', 'taras', 'loggia']),
        "has_garage": any(word in text_lower for word in ['gara≈º', 'parking', 'miejsce parkingowe']),
        "has_garden": any(word in text_lower for word in ['ogr√≥d', 'ogr√≥dek', 'dzia≈Çka']),
        "has_elevator": any(word in text_lower for word in ['winda', 'elevator', 'ascensor'])
    }

def determine_market_type(text: str, url: str) -> str:
    """Okre≈õla typ rynku (pierwotny/wt√≥rny) na podstawie tekstu i URL"""
    text_lower = text.lower()
    url_lower = url.lower()
    
    # S≈Çowa kluczowe dla rynku pierwotnego
    primary_keywords = [
        'nowe', 'nowy', 'deweloper', 'inwestycja', 'przedsprzeda≈º', 
        'nowa inwestycja', 'od developera', 'stan deweloperski',
        'pierwszy w≈Ça≈õciciel', 'pierwotny'
    ]
    
    # S≈Çowa kluczowe dla rynku wt√≥rnego
    secondary_keywords = [
        'wt√≥rny', 'u≈ºywane', 'do remontu', 'po remoncie',
        'mieszkanie w≈Çasno≈õciowe', 'z drugiej rƒôki'
    ]
    
    # Sprawd≈∫ rynek pierwotny
    if any(keyword in text_lower for keyword in primary_keywords):
        return 'pierwotny'
    
    # Sprawd≈∫ rynek wt√≥rny
    if any(keyword in text_lower for keyword in secondary_keywords):
        return 'wt√≥rny'
    
    # Domy≈õlnie przyjmij rynek wt√≥rny (czƒô≈õciej wystƒôpuje)
    return 'wt√≥rny'

def get_otodom_listings(base_url: str = DEFAULT_BASE_URL,
                        max_pages: Optional[int] = 0,
                        scrape_details: bool = True,
                        batch_size: int = 100,
                        batch_callback: Optional[Callable[[List[Dict]], None]] = None,
                        resume: bool = False,
                        max_workers: int = 4,
                        enable_geocoding: bool = True) -> List[Dict]:
    """
    Pobiera og≈Çoszenia z Otodom.pl z opcjonalnym scrapingiem szczeg√≥≈Ç√≥w
    
    Args:
        base_url: Podstawowy URL do strony wynik√≥w
        max_pages: (opcjonalnie) maksymalna liczba stron do przeskanowania. 
                   Je≈õli None lub <=0, scraper bƒôdzie przechodzi≈Ç kolejne strony 
                   a≈º do wykrycia ostatniej (brak og≈Çosze≈Ñ lub brak przycisku "Nastƒôpna strona").
        scrape_details: Czy wchodziƒá w szczeg√≥≈Çy ka≈ºdego og≈Çoszenia
        batch_size: Rozmiar batcha do zapisu do bazy
        batch_callback: Funkcja do zapisu batcha do bazy
        resume: Czy kontynuowaƒá od ostatniego punktu zapisu (domy≈õlnie wy≈ÇƒÖczone)
        max_workers: Liczba wƒÖtk√≥w do wielowƒÖtkowego scrapowania szczeg√≥≈Ç√≥w (domy≈õlnie 4)
        enable_geocoding: Czy pobieraƒá wsp√≥≈Çrzƒôdne geograficzne podczas scrapowania (domy≈õlnie False)
    
    Returns:
        List[Dict]: Lista og≈Çosze≈Ñ
    """
    listings = []
    
    # Plik z checkpointami (np. ~/.otodom_progress.json)
    progress_file = Path.home() / ".otodom_progress.json"
    if resume and progress_file.exists():
        try:
            progress_data = json.loads(progress_file.read_text(encoding="utf-8"))
        except Exception:
            progress_data = {}
    else:
        progress_data = {}

    progress_key = base_url
    start_page = int(progress_data.get(progress_key, 1))

    page = start_page
    while True:
        # Sprawd≈∫ limit je≈õli podano dodatniƒÖ liczbƒô stron
        if max_pages is not None and max_pages > 0 and page > max_pages:
            logger.info("üèÅ OsiƒÖgniƒôto maksymalnƒÖ liczbƒô stron okre≈õlonƒÖ przez u≈ºytkownika")
            break
        
        try:
            # Konstruuj URL z parametrami
            if page == 1:
                url = f"{base_url}?viewType=listing"
            else:
                url = f"{base_url}?viewType=listing&page={page}"
                
            logger.info(f"üè† Scrapujƒô Otodom.pl - strona {page}")
            logger.info(f"üîó URL: {url}")
            
            # U≈ºywamy Selenium dla Otodom
            soup = get_soup(url, use_selenium=True)
            
            # Selektory dla kontener√≥w og≈Çosze≈Ñ
            offers = (soup.select("[data-cy='listing-item']") or 
                     soup.select("article.css-136g1q2") or 
                     soup.select("article") or
                     soup.select(".listing-item"))
            
            # POPRAWIONA LOGIKA WYKRYWANIA KO≈ÉCA STRON
            if not offers:
                logger.warning(f"‚ö†Ô∏è Nie znaleziono og≈Çosze≈Ñ na stronie {page}")
                
                # Sprawd≈∫ czy strona za≈Çadowa≈Ça siƒô prawid≈Çowo
                if "otodom" not in soup.get_text().lower():
                    logger.error("‚ùå Strona nie za≈Çadowa≈Ça siƒô prawid≈Çowo - pr√≥bujƒô ponownie")
                    # Dodaj kr√≥tkie op√≥≈∫nienie i spr√≥buj ponownie
                    time.sleep(5)
                    continue
                
                # Sprawd≈∫ czy jest informacja o ko≈Ñcu wynik√≥w
                page_text = soup.get_text().lower()
                end_indicators = [
                    "brak wynik√≥w",
                    "nie znaleziono", 
                    "koniec wynik√≥w",
                    "strona nie istnieje",
                    "404",
                    "b≈ÇƒÖd"
                ]
                
                if any(indicator in page_text for indicator in end_indicators):
                    logger.info(f"üèÅ Wykryto koniec wynik√≥w na stronie {page}")
                    break
                
                # Sprawd≈∫ czy istniejƒÖ przyciski nawigacji
                pagination_elements = soup.select("nav, .pagination, [data-cy*='pagination'], a[title*='nastƒôpna'], a[title*='dalej']")
                
                if not pagination_elements:
                    logger.info(f"üèÅ Brak element√≥w paginacji na stronie {page} - koniec")
                    break
                
                # Je≈õli nie ma wyra≈∫nych wska≈∫nik√≥w ko≈Ñca, spr√≥buj jeszcze kilka stron
                if page <= 5:  # Dla pierwszych 5 stron - mo≈ºe byƒá przej≈õciowy b≈ÇƒÖd
                    logger.warning(f"‚ö†Ô∏è Strona {page} pusta, ale kontynuujƒô (mo≈ºe byƒá przej≈õciowy b≈ÇƒÖd)")
                    page += 1
                    continue
                else:
                    logger.info(f"üèÅ Brak og≈Çosze≈Ñ na stronie {page} - prawdopodobnie koniec wynik√≥w")
                    break
            
            logger.info(f"üìã Znaleziono {len(offers)} og≈Çosze≈Ñ na stronie {page}")
            
            # Dodaj sprawdzenie czy liczba og≈Çosze≈Ñ znacznie spad≈Ça
            if page > 3 and len(offers) < 10:  # Je≈õli po 3 stronie mniej ni≈º 10 og≈Çosze≈Ñ
                logger.warning(f"‚ö†Ô∏è Znaczny spadek liczby og≈Çosze≈Ñ na stronie {page} ({len(offers)})")
                
                # Sprawd≈∫ czy to rzeczywi≈õcie koniec
                total_items_text = soup.get_text()
                if "wynik" in total_items_text.lower():
                    # Spr√≥buj wyciƒÖgnƒÖƒá informacjƒô o ≈ÇƒÖcznej liczbie wynik√≥w
                    import re
                    matches = re.findall(r'(\d+)\s*wynik', total_items_text.lower())
                    if matches:
                        total_results = int(matches[0])
                        expected_pages = (total_results // 24) + 1
                        logger.info(f"üìä Znaleziono informacjƒô o {total_results} wynikach, oczekiwane strony: {expected_pages}")
                        
                        if page > expected_pages:
                            logger.info(f"üèÅ Przekroczono oczekiwanƒÖ liczbƒô stron ({page} > {expected_pages})")
                            break
            
            # KROK 1: Sparsuj wszystkie podstawowe dane z tej strony
            page_listings = []
            for i, offer in enumerate(offers):
                try:
                    listing = parse_otodom_listing(offer)
                    if listing:
                        listing["source"] = "otodom.pl"
                        listing["source_page"] = page
                        listing["source_position"] = i + 1
                        page_listings.append(listing)
                        logger.debug(f"‚úÖ Parsowano podstawowe dane {i+1}: {listing.get('title_raw', '')[:30]}...")
                except Exception as e:
                    logger.error(f"‚ùå B≈ÇƒÖd parsowania podstawowych danych og≈Çoszenia {i+1}: {e}")
            
            # KROK 2: Je≈õli scrape_details=True, pobierz szczeg√≥≈Çy WIELOWƒÑTKOWO
            if scrape_details and page_listings:
                geocoding_status = "z geocodingiem ‚úÖ" if enable_geocoding and GEOCODING_AVAILABLE else "bez geocodingu ‚ö†Ô∏è"
                logger.info(f"üöÄ Rozpoczynam wielowƒÖtkowe pobieranie szczeg√≥≈Ç√≥w dla {len(page_listings)} og≈Çosze≈Ñ ({max_workers} wƒÖtk√≥w, {geocoding_status})")
                
                # Ustaw licznik postƒôpu
                with _progress_lock:
                    _progress_counter["current"] = 0
                    _progress_counter["total"] = len(page_listings)
                
                # U≈ºyj ThreadPoolExecutor z konfigurowalnƒÖ liczbƒÖ wƒÖtk√≥w
                with ThreadPoolExecutor(max_workers=max_workers, thread_name_prefix="OtodomScraper") as executor:
                    # Wy≈õlij wszystkie zadania
                    future_to_listing = {
                        executor.submit(scrape_listing_details_thread_safe, listing.copy(), enable_geocoding): listing 
                        for listing in page_listings
                    }
                    
                    # Zbierz wyniki w miarƒô ich gotowo≈õci
                    completed_listings = []
                    for future in as_completed(future_to_listing):
                        try:
                            completed_listing = future.result(timeout=60)  # 60s timeout per listing
                            completed_listings.append(completed_listing)
                        except Exception as e:
                            original_listing = future_to_listing[future]
                            logger.error(f"‚ùå B≈ÇƒÖd w wƒÖtku dla {original_listing.get('url', 'unknown')}: {e}")
                            # Dodaj podstawowe dane bez szczeg√≥≈Ç√≥w
                            completed_listings.append(original_listing)
                
                # ZastƒÖp podstawowe listingi uzupe≈Çnionymi
                page_listings = completed_listings
                logger.info(f"‚úÖ Zako≈Ñczono wielowƒÖtkowe pobieranie szczeg√≥≈Ç√≥w dla strony {page}")
            
            # KROK 3: Dodaj do g≈Ç√≥wnej listy i obs≈Çu≈º batche
            for listing in page_listings:
                listings.append(listing)
                
                # Batch zapisu
                if batch_size and len(listings) >= batch_size and batch_callback:
                    logger.info("üíæ OsiƒÖgniƒôto wielko≈õƒá batcha ‚Äì zapisujƒô do bazy‚Ä¶")
                    try:
                        batch_callback(listings)
                    finally:
                        listings.clear()
            
            random_delay()
            
            # Je≈õli nie znaleziono ≈ºadnych ofert na kolejnej stronie, zako≈Ñcz pƒôtlƒô
            # (sprawdzenie bƒôdzie wykonane na poczƒÖtku kolejnej iteracji)

        except Exception as e:
            logger.error(f"‚ùå B≈ÇƒÖd pobierania strony {page}: {e}")
            
            # LEPSZA OBS≈ÅUGA B≈ÅƒòD√ìW
            error_msg = str(e).lower()
            
            # Sprawd≈∫ czy to b≈ÇƒÖd tymczasowy
            temporary_errors = [
                "timeout", "connection", "network", "ssl", "http",
                "502", "503", "504", "connection reset", "connection refused"
            ]
            
            if any(temp_error in error_msg for temp_error in temporary_errors):
                logger.warning(f"‚ö†Ô∏è B≈ÇƒÖd tymczasowy na stronie {page}, czekam 10 sekund i pr√≥bujƒô ponownie...")
                time.sleep(10)
                continue
            
            # Sprawd≈∫ czy to b≈ÇƒÖd blokady (anti-bot)
            blocking_errors = [
                "403", "forbidden", "blocked", "bot", "robot", "captcha"
            ]
            
            if any(block_error in error_msg for block_error in blocking_errors):
                logger.error(f"üö´ Prawdopodobna blokada anti-bot na stronie {page}")
                logger.info("üí° Sugestia: Zwiƒôksz op√≥≈∫nienia miƒôdzy ≈ºƒÖdaniami lub u≈ºyj proxy")
                break
            
            # Dla innych b≈Çƒôd√≥w - przejd≈∫ do nastƒôpnej strony ale nie przerywaj ca≈Çkowicie
            logger.warning(f"‚ö†Ô∏è Pomijam stronƒô {page} z powodu b≈Çƒôdu: {e}")
            
            # Je≈õli jest zbyt wiele b≈Çƒôd√≥w pod rzƒÖd, przerwij
            if not hasattr(get_otodom_listings, 'consecutive_errors'):
                consecutive_errors = 0
            else:
                consecutive_errors = getattr(get_otodom_listings, 'consecutive_errors', 0)
                
            consecutive_errors += 1
            setattr(get_otodom_listings, 'consecutive_errors', consecutive_errors)
            
            if consecutive_errors >= 3:
                logger.error(f"‚ùå Zbyt wiele b≈Çƒôd√≥w pod rzƒÖd ({consecutive_errors}), przerywam")
                break
                
            # Op√≥≈∫nienie po b≈Çƒôdzie
            time.sleep(5)
            
        # Zapisz progres
        if resume:
            progress_data[progress_key] = page
            try:
                progress_file.write_text(json.dumps(progress_data, ensure_ascii=False), encoding="utf-8")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Nie uda≈Ço siƒô zapisaƒá checkpointu: {e}")

        page += 1

    # Resetuj checkpoint po zako≈Ñczeniu
    if resume and progress_key in progress_data:
        progress_data.pop(progress_key, None)
        try:
            progress_file.write_text(json.dumps(progress_data, ensure_ascii=False), encoding="utf-8")
        except Exception:
            pass

    logger.info(f"‚úÖ Pobrano ≈ÅƒÑCZNIE {len(listings)} og≈Çosze≈Ñ z Otodom.pl")
    return listings

def parse_otodom_listing(offer_element) -> Dict:
    """
    Parsuje pojedyncze og≈Çoszenie z Otodom.pl zgodnie z nowƒÖ strukturƒÖ bazy
    
    Args:
        offer_element: Element BeautifulSoup z og≈Çoszeniem
    
    Returns:
        Dict: Dane og≈Çoszenia zgodne z nowƒÖ strukturƒÖ bazy
    """
    # TYTU≈Å (title_raw)
    title_elem = (offer_element.select_one("[data-cy='listing-item-title']") or
                  offer_element.select_one("p.css-u3orbr") or
                  offer_element.select_one("h3") or
                  offer_element.select_one("h2"))
    title_raw = clean_text(title_elem.get_text()) if title_elem else ""
    
    # CENA
    price_elem = (offer_element.select_one("span.css-2bt9f1") or
                  offer_element.select_one("[data-sentry-element='Content']") or
                  offer_element.select_one("[data-cy*='price']"))
    price_text = clean_text(price_elem.get_text()) if price_elem else ""
    price_data = extract_price(price_text)
    
    # LOKALIZACJA (address_raw)
    location_elem = (offer_element.select_one("p.css-42r2ms") or
                     offer_element.select_one("[data-sentry-element='StyledParagraph']") or
                     offer_element.select_one("[data-cy='listing-item-location']"))
    address_raw = clean_text(location_elem.get_text()) if location_elem else ""
    
    # LINK (URL)
    link_elem = (offer_element.select_one("[data-cy='listing-item-link']") or
                 offer_element.select_one("a[href*='/oferta/']") or
                 offer_element.select_one("a"))
    url = link_elem.get("href") if link_elem else ""
    if url and not url.startswith("http"):
        url = f"https://www.otodom.pl{url}"
    
    # POWIERZCHNIA I POKOJE z <dl> struktury
    area_value = None
    rooms_value = None
    
    specs_list = offer_element.select_one("dl.css-9q2yy4")
    if specs_list:
        dt_elements = specs_list.select("dt")
        dd_elements = specs_list.select("dd")
        
        for i, dt in enumerate(dt_elements):
            dt_text = clean_text(dt.get_text())
            if i < len(dd_elements):
                dd_text = clean_text(dd_elements[i].get_text())
                
                if "powierzchnia" in dt_text.lower():
                    area_value = extract_numeric_value(dd_text)
                elif "pokoi" in dt_text.lower() or "pok√≥j" in dt_text.lower():
                    rooms_value = extract_numeric_value(dd_text)
    
    # Fallback dla powierzchni
    if not area_value:
        area_elem = offer_element.select_one("span:contains('m¬≤')")
        if area_elem:
            area_value = extract_numeric_value(area_elem.get_text())
    
    # DODATKOWE SZCZEG√ì≈ÅY z sekcji AdDetails
    additional_details = extract_detailed_features(offer_element)
    
    # Sprawd≈∫ czy mamy podstawowe dane
    if not title_raw and not url:
        return None
    
    # Kombinowany tekst do analizy cech
    combined_text = f"{title_raw} {address_raw}".lower()
    
    # Wydobyj cechy boolean
    boolean_features = extract_boolean_features(combined_text)
    
    # Po≈ÇƒÖcz z dodatkowymi cechami ze szczeg√≥≈Ç√≥w
    boolean_features.update(additional_details['boolean_features'])
    
    # Okre≈õl typ rynku
    market_type = determine_market_type(combined_text, url)
    if additional_details.get('market'):
        market_type = additional_details['market']
    
    # Parsowanie zaawansowanych komponent√≥w adresu
    address_components = parse_address_components(address_raw)
    
    # WyciƒÖgnij komponenty
    street = address_components.get("street")
    district = address_components.get("district") 
    city = address_components.get("city")
    province = address_components.get("province")
    
    # Struktura danych zgodna z nowƒÖ bazƒÖ danych
    listing = {
        # Podstawowe informacje
        "url": url,
        "listing_id": None,  # Bƒôdzie uzupe≈Çnione ze szczeg√≥≈Ç√≥w
        "title_raw": title_raw,
        "address_raw": address_raw,
        
        # Cena i powierzchnia
        "price": price_data["price"],
        "area": area_value,
        "rooms": int(rooms_value) if rooms_value and rooms_value > 0 else None,
        
        # Lokalizacja (sparsowane komponenty)
        "city": city,
        "district": district,
        "street": street,
        "province": province,
        
        # Typ rynku
        "market": market_type,
        
        # Cechy boolean - podstawowe + szczeg√≥≈Çowe
        "has_balcony": boolean_features.get("has_balcony", False),
        "has_garage": boolean_features.get("has_garage", False), 
        "has_garden": boolean_features.get("has_garden", False),
        "has_elevator": boolean_features.get("has_elevator", False),
        
        # Dodatkowe szczeg√≥≈Çy budynku
        "year_of_construction": additional_details.get("year_of_construction"),
        "building_type": additional_details.get("building_type"),
        "floor": additional_details.get("floor"),
        "total_floors": additional_details.get("total_floors"),
        "standard_of_finish": additional_details.get("standard_of_finish"),
        
        # Pola kt√≥re wymagajƒÖ dodatkowych danych (na razie None)
        "listing_date": None,
        "latitude": None,
        "longitude": None,
        "distance_to_city_center": None,
        "distance_to_nearest_school": None,
        "distance_to_nearest_kindergarten": None,
        "distance_to_nearest_public_transport": None,
        "distance_to_nearest_supermarket": None,
        
        # Metadane
        "source": "otodom.pl"
    }
    
    return listing

def extract_detailed_features(offer_element) -> Dict:
    """
    Ekstraktuje szczeg√≥≈Çowe cechy z sekcji AdDetails
    Na podstawie struktury: div[data-sentry-component="AdDetailsBase"]
    
    Args:
        offer_element: Element BeautifulSoup z og≈Çoszeniem
    
    Returns:
        Dict z dodatkowymi cechami
    """
    result = {
        'boolean_features': {
            'has_balcony': False,
            'has_garage': False,
            'has_garden': False,
            'has_elevator': False,
            'has_basement': False,
            'has_separate_kitchen': False
        },
        'market': None,
        'year_of_construction': None,
        'building_type': None,
        'floor': None,
        'total_floors': None,
        'standard_of_finish': None,
        'heating_type': None,
        'rent_amount': None
    }
    
    # Szukaj sekcji szczeg√≥≈Ç√≥w
    details_container = offer_element.select_one('[data-sentry-component="AdDetailsBase"]')
    if not details_container:
        return result
    
    # Wszystkie pary klucz-warto≈õƒá z sekcji szczeg√≥≈Ç√≥w
    item_containers = details_container.select('[data-sentry-source-file="AdDetailItem.tsx"].css-1xw0jqp')
    
    for container in item_containers:
        # Znajd≈∫ etykietƒô i warto≈õƒá
        label_elem = container.select_one('p.esen0m92.css-1airkmu:first-child')
        value_elem = container.select_one('p.esen0m92.css-1airkmu:nth-child(2), p.esen0m92.css-wcoypf')
        
        if not label_elem or not value_elem:
            continue
            
        label = clean_text(label_elem.get_text()).lower()
        value = clean_text(value_elem.get_text()).lower()
        
        # Mapowanie p√≥l
        if 'powierzchnia' in label:
            # Ju≈º obs≈Çugiwane w g≈Ç√≥wnej funkcji
            pass
        elif 'liczba pokoi' in label:
            # Ju≈º obs≈Çugiwane w g≈Ç√≥wnej funkcji
            pass
        elif 'piƒôtro' in label:
            # Format: "3/4" -> floor=3, total_floors=4
            if '/' in value:
                parts = value.split('/')
                try:
                    result['floor'] = int(parts[0].strip())
                    result['total_floors'] = int(parts[1].strip())
                except (ValueError, IndexError):
                    pass
        elif 'rok budowy' in label:
            try:
                year = int(value.strip())
                if 1800 <= year <= 2030:  # Walidacja rozsƒÖdnych lat
                    result['year_of_construction'] = year
            except ValueError:
                pass
        elif 'winda' in label:
            result['boolean_features']['has_elevator'] = 'tak' in value or 'yes' in value
        elif 'rodzaj zabudowy' in label:
            # blok, kamienica, dom wolnostojƒÖcy, itp.
            result['building_type'] = value
        elif 'stan wyko≈Ñczenia' in label:
            result['standard_of_finish'] = value
        elif 'ogrzewanie' in label:
            result['heating_type'] = value
        elif 'czynsz' in label:
            # WyciƒÖgnij kwotƒô czynszu
            rent_value = extract_numeric_value(value)
            if rent_value and rent_value > 0:
                result['rent_amount'] = rent_value
        elif 'rynek' in label:
            if 'pierwotny' in value:
                result['market'] = 'pierwotny'
            elif 'wt√≥rny' in value:
                result['market'] = 'wt√≥rny'
    
    # Szukaj cech boolean w sekcji "Informacje dodatkowe"
    additional_info_containers = details_container.select('span.css-axw7ok.esen0m94')
    for span in additional_info_containers:
        feature_text = clean_text(span.get_text()).lower()
        
        if 'balkon' in feature_text:
            result['boolean_features']['has_balcony'] = True
        elif 'gara≈º' in feature_text or 'parking' in feature_text:
            result['boolean_features']['has_garage'] = True
        elif 'ogr√≥d' in feature_text or 'dzia≈Çka' in feature_text:
            result['boolean_features']['has_garden'] = True
        elif 'piwnica' in feature_text:
            result['boolean_features']['has_basement'] = True
        elif 'oddzielna kuchnia' in feature_text:
            result['boolean_features']['has_separate_kitchen'] = True
        elif 'winda' in feature_text:
            result['boolean_features']['has_elevator'] = True
    
    return result

def scrape_individual_listing(url: str) -> Dict:
    """
    Scrapuje szczeg√≥≈Çowe dane z indywidualnej strony og≈Çoszenia
    
    Args:
        url: URL do strony og≈Çoszenia
    
    Returns:
        Dict: Szczeg√≥≈Çowe dane z og≈Çoszenia
    """
    try:
        # Pobierz stronƒô og≈Çoszenia
        soup = get_soup(url, use_selenium=True)
        
        if not soup:
            logger.error(f"‚ùå Nie uda≈Ço siƒô za≈Çadowaƒá strony: {url}")
            return {}
        
        # Dodaj kr√≥tkie op√≥≈∫nienie miƒôdzy requestami
        random_delay()
        
        # Struktura wynikowa
        detailed_data = {
            "year_of_construction": None,
            "building_type": None,
            "floor": None,
            "total_floors": None,
            "standard_of_finish": None,
            "heating_type": None,
            "rent_amount": None,
            "has_balcony": False,
            "has_garage": False,
            "has_garden": False,
            "has_elevator": False,
            "has_basement": False,
            "has_separate_kitchen": False,
            "has_dishwasher": False,
            "has_fridge": False,
            "has_oven": False,
            "security_features": [],
            "media_features": []
        }
        
        # Znajd≈∫ sekcjƒô AdDetails
        details_container = soup.select_one('[data-sentry-component="AdDetailsBase"]')
        if not details_container:
            # Fallback - szukaj innych kontener√≥w ze szczeg√≥≈Çami
            details_container = soup.select_one('.css-8mnxk5') or soup
        
        # Parsuj szczeg√≥≈Çy z par klucz-warto≈õƒá
        item_containers = details_container.select('[data-sentry-source-file="AdDetailItem.tsx"].css-1xw0jqp')
        
        for container in item_containers:
            # Znajd≈∫ etykietƒô i warto≈õƒá
            label_elem = container.select_one('p.esen0m92.css-1airkmu:first-child')
            value_elem = container.select_one('p.esen0m92.css-1airkmu:nth-child(2), p.esen0m92.css-wcoypf')
            
            if not label_elem or not value_elem:
                continue
                
            label = clean_text(label_elem.get_text()).lower()
            value = clean_text(value_elem.get_text()).lower()
            
            # Mapowanie p√≥l szczeg√≥≈Çowych
            if 'piƒôtro' in label:
                # Format: "3/4" -> floor=3, total_floors=4
                if '/' in value:
                    parts = value.split('/')
                    try:
                        detailed_data['floor'] = int(parts[0].strip())
                        detailed_data['total_floors'] = int(parts[1].strip())
                    except (ValueError, IndexError):
                        pass
            elif 'rok budowy' in label:
                try:
                    year = int(value.strip())
                    if 1800 <= year <= 2030:  # Walidacja rozsƒÖdnych lat
                        detailed_data['year_of_construction'] = year
                except ValueError:
                    pass
            elif 'winda' in label:
                detailed_data['has_elevator'] = 'tak' in value or 'yes' in value
            elif 'rodzaj zabudowy' in label:
                # Mapuj na enum warto≈õci z bazy
                building_mapping = {
                    'blok': 'blok',
                    'kamienica': 'kamienica', 
                    'apartamentowiec': 'apartamentowiec',
                    'dom wielorodzinny': 'dom wielorodzinny',
                    'wielka p≈Çyta': 'wielka p≈Çyta'
                }
                for key, mapped_value in building_mapping.items():
                    if key in value:
                        detailed_data['building_type'] = mapped_value
                        break
                if not detailed_data['building_type']:
                    detailed_data['building_type'] = 'inny'
            elif 'stan wyko≈Ñczenia' in label:
                # Mapuj na liczby (standard_of_finish to tinyint)
                finish_mapping = {
                    'do zamieszkania': 1,
                    'gotowe do zamieszkania': 1,
                    'developerski': 2,
                    'deweloperski': 2,
                    'do wyko≈Ñczenia': 3,
                    'do remontu': 4,
                    'surowy otwarty': 5,
                    'surowy zamkniƒôty': 6
                }
                for key, mapped_value in finish_mapping.items():
                    if key in value:
                        detailed_data['standard_of_finish'] = mapped_value
                        break
            elif 'ogrzewanie' in label:
                detailed_data['heating_type'] = value
            elif 'czynsz' in label:
                # WyciƒÖgnij kwotƒô czynszu
                rent_value = extract_numeric_value(value)
                if rent_value and rent_value > 0:
                    detailed_data['rent_amount'] = rent_value
            elif 'rynek' in label:
                # To jest ju≈º parsowane na poziomie listing, ale dla pewno≈õci
                if 'pierwotny' in value:
                    detailed_data['market'] = 'pierwotny'
                elif 'wt√≥rny' in value:
                    detailed_data['market'] = 'wt√≥rny'
        
        # Parsuj cechy boolean z sekcji "Informacje dodatkowe"
        additional_info_containers = details_container.select('span.css-axw7ok.esen0m94')
        for span in additional_info_containers:
            feature_text = clean_text(span.get_text()).lower()
            
            if 'balkon' in feature_text:
                detailed_data['has_balcony'] = True
            elif 'gara≈º' in feature_text or 'parking' in feature_text:
                detailed_data['has_garage'] = True
            elif 'ogr√≥d' in feature_text or 'dzia≈Çka' in feature_text:
                detailed_data['has_garden'] = True
            elif 'piwnica' in feature_text:
                detailed_data['has_basement'] = True
            elif 'oddzielna kuchnia' in feature_text:
                detailed_data['has_separate_kitchen'] = True
            elif 'winda' in feature_text:
                detailed_data['has_elevator'] = True
            elif 'zmywarka' in feature_text:
                detailed_data['has_dishwasher'] = True
            elif 'lod√≥wka' in feature_text:
                detailed_data['has_fridge'] = True
            elif 'piekarnik' in feature_text:
                detailed_data['has_oven'] = True
        
        # Pobierz ID og≈Çoszenia z sekcji opisu
        try:
            # Szukaj ID w sekcji z opisem - u≈ºywamy dok≈Çadnego selektora z przyk≈Çadu
            id_element = soup.select_one('p.e1izz2zk2.css-htq2ld')
            if id_element and 'ID:' in id_element.get_text():
                id_text = id_element.get_text()
                # WyciƒÖgnij samo ID (np. z "ID: 66708040")
                id_match = re.search(r'ID:\s*(\d+)', id_text)
                if id_match:
                    detailed_data['listing_id'] = id_match.group(1)
                    logger.debug(f"‚úÖ Pobrano ID og≈Çoszenia: {detailed_data['listing_id']}")
            else:
                # Fallback - szukaj innych mo≈ºliwych selektor√≥w ID
                fallback_selectors = [
                    "p:contains('ID:')",
                    "[data-cy*='id']",
                    "*:contains('ID:')"
                ]
                for selector in fallback_selectors:
                    try:
                        id_element = soup.select_one(selector)
                        if id_element and 'ID:' in id_element.get_text():
                            id_text = id_element.get_text()
                            id_match = re.search(r'ID:\s*(\d+)', id_text)
                            if id_match:
                                detailed_data['listing_id'] = id_match.group(1)
                                logger.debug(f"‚úÖ Pobrano ID (fallback): {detailed_data['listing_id']}")
                                break
                    except:
                        continue
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Nie uda≈Ço siƒô pobraƒá ID og≈Çoszenia: {e}")
        
        # Parsuj sekcje rozwijane (Wyposa≈ºenie, Zabezpieczenia, Media)
        parse_equipment_sections(soup, detailed_data)
        
        logger.debug(f"‚úÖ Szczeg√≥≈Çy pobrane: {sum(1 for v in detailed_data.values() if v)} p√≥l wype≈Çnionych")
        return detailed_data
        
    except Exception as e:
        logger.error(f"‚ùå B≈ÇƒÖd scrapingu szczeg√≥≈Ç√≥w {url}: {e}")
        return {}

def parse_equipment_sections(soup, detailed_data: Dict):
    """
    Parsuje sekcje wyposa≈ºenia, zabezpiecze≈Ñ i medi√≥w z rozwijanych accordion√≥w
    
    Args:
        soup: BeautifulSoup object strony
        detailed_data: S≈Çownik do aktualizacji danymi
    """
    try:
        # Znajd≈∫ wszystkie sekcje accordion
        accordion_sections = soup.select('[data-isopen="false"] .n-accordionitem-content, [data-isopen="true"] .n-accordionitem-content')
        
        for section in accordion_sections:
            # Sprawd≈∫ content sekcji
            section_text = clean_text(section.get_text()).lower()
            
            # Parsuj cechy z wyposa≈ºenia
            equipment_spans = section.select('span.css-axw7ok.esen0m94')
            for span in equipment_spans:
                feature_text = clean_text(span.get_text()).lower()
                
                # Wyposa≈ºenie AGD
                if 'zmywarka' in feature_text:
                    detailed_data['has_dishwasher'] = True
                elif 'lod√≥wka' in feature_text:
                    detailed_data['has_fridge'] = True
                elif 'piekarnik' in feature_text:
                    detailed_data['has_oven'] = True
                
                # Zabezpieczenia
                elif 'antyw≈Çamaniowe' in feature_text or 'drzwi antyw≈Çamaniowe' in feature_text:
                    if 'security_features' not in detailed_data:
                        detailed_data['security_features'] = []
                    detailed_data['security_features'].append('drzwi_antywlamaniowe')
                elif 'domofon' in feature_text or 'wideofon' in feature_text:
                    if 'security_features' not in detailed_data:
                        detailed_data['security_features'] = []
                    detailed_data['security_features'].append('domofon')
                
                # Media
                elif 'internet' in feature_text:
                    if 'media_features' not in detailed_data:
                        detailed_data['media_features'] = []
                    detailed_data['media_features'].append('internet')
                elif 'telewizja kablowa' in feature_text:
                    if 'media_features' not in detailed_data:
                        detailed_data['media_features'] = []
                    detailed_data['media_features'].append('tv_kablowa')
                elif 'telefon' in feature_text:
                    if 'media_features' not in detailed_data:
                        detailed_data['media_features'] = []
                    detailed_data['media_features'].append('telefon')
    
    except Exception as e:
        logger.debug(f"‚ö†Ô∏è B≈ÇƒÖd parsowania sekcji wyposa≈ºenia: {e}")

def parse_address_components(address_raw: str) -> Dict[str, str]:
    """
    Parsuje adres na komponenty: ulica, dzielnica, miasto, wojew√≥dztwo
    
    Przyk≈Çady format√≥w z Otodom:
    - "ul. Kanarkowa, Gutkowo, Olsztyn, warmi≈Ñsko-mazurskie"
    - "ul. Jana Boenigka, Jaroty, Olsztyn, warmi≈Ñsko-mazurskie"
    - "Tƒôczowy Las, Osiedle Genera≈Ç√≥w, Olsztyn, warmi≈Ñsko-mazurskie"
    - "ul. Franciszka Hynka, Dywity, Dywity, olszty≈Ñski, warmi≈Ñsko-mazurskie"
    
    Args:
        address_raw: Surowy adres z portalu
        
    Returns:
        Dict z komponentami adresu: {street, district, city, province}
    """
    if not address_raw:
        return {"street": None, "district": None, "city": None, "province": None}
    
    # Podziel na czƒô≈õci po przecinkach
    parts = [part.strip() for part in address_raw.split(',') if part.strip()]
    
    if not parts:
        return {"street": None, "district": None, "city": None, "province": None}
    
    result = {"street": None, "district": None, "city": None, "province": None}
    
    # Ostatnia czƒô≈õƒá to zazwyczaj wojew√≥dztwo
    if len(parts) >= 1:
        result["province"] = parts[-1]
    
    # Druga od ko≈Ñca to zazwyczaj miasto g≈Ç√≥wne lub powiat
    if len(parts) >= 2:
        potential_city = parts[-2]
        
        # Sprawd≈∫ czy to nie jest powiat (ko≈ÑczƒÖcy siƒô na -ski/-≈Ñski)
        if potential_city.endswith(('ski', '≈Ñski', 'cki', 'dzki')):
            # To jest powiat, miasto mo≈ºe byƒá wcze≈õniej
            if len(parts) >= 3:
                result["city"] = parts[-3]
        else:
            result["city"] = potential_city
    
    # Trzecia od ko≈Ñca to zazwyczaj dzielnica/osiedle
    if len(parts) >= 3:
        potential_district = parts[-3]
        
        # Je≈õli miasto nie zosta≈Ço jeszcze ustalone
        if not result["city"]:
            result["city"] = potential_district
        else:
            result["district"] = potential_district
    
    # Pierwsza czƒô≈õƒá to zazwyczaj ulica
    if len(parts) >= 1:
        potential_street = parts[0]
        
        # Sprawd≈∫ czy zawiera typowe prefiks ulicy
        street_prefixes = ['ul.', 'al.', 'pl.', 'os.', 'ul', 'al', 'pl', 'os']
        has_street_prefix = any(potential_street.lower().startswith(prefix) for prefix in street_prefixes)
        
        if has_street_prefix or 'ul.' in potential_street.lower():
            result["street"] = potential_street
        elif len(parts) == 4 and not result["district"]:
            # Je≈õli nie ma prefiksu, ale mamy 4 czƒô≈õci, mo≈ºe to byƒá nazwa osiedla/ulicy
            result["street"] = potential_street
        elif len(parts) >= 4 and not result["district"]:
            # Mo≈ºe to byƒá dzielnica
            result["district"] = potential_street
    
    # Dodatkowa logika dla przypadk√≥w specjalnych
    if len(parts) == 4:
        # Format: "ulica, dzielnica, miasto, wojew√≥dztwo"
        result["street"] = parts[0]
        result["district"] = parts[1]
        result["city"] = parts[2]
        result["province"] = parts[3]
    elif len(parts) == 5:
        # Format: "ulica, dzielnica, miasto, powiat, wojew√≥dztwo"
        result["street"] = parts[0]
        result["district"] = parts[1]
        result["city"] = parts[2]
        # parts[3] to powiat - pomijamy
        result["province"] = parts[4]
    
    # Walidacja i oczyszczenie
    for key in result:
        if result[key]:
            result[key] = result[key].strip()
            # Usu≈Ñ puste stringi
            if result[key] == "":
                result[key] = None
    
    return result

if __name__ == "__main__":
    """Test scrapera"""
    print("üß™ TEST SCRAPERA OTODOM.PL - WIELOWƒÑTKOWA WERSJA")
    print("="*60)
    
    try:
        # Test z wielowƒÖtkowo≈õciƒÖ (4 wƒÖtki) + geocoding
        start_time = time.time()
        listings = get_otodom_listings(max_pages=0, max_workers=4, enable_geocoding=True)
        end_time = time.time()
        execution_time = end_time - start_time
        
        if listings:
            print(f"‚úÖ Pobrano {len(listings)} og≈Çosze≈Ñ w {execution_time:.2f} sekund")
            print(f"‚ö° ≈öredni czas na og≈Çoszenie: {execution_time/len(listings):.2f}s")
            
            # Statystyki
            with_price = len([l for l in listings if l.get('price')])
            with_location = len([l for l in listings if l.get('address_raw')])
            with_area = len([l for l in listings if l.get('area')])
            with_rooms = len([l for l in listings if l.get('rooms')])
            
            # Statystyki cech boolean
            with_balcony = len([l for l in listings if l.get('has_balcony')])
            with_garage = len([l for l in listings if l.get('has_garage')])
            with_garden = len([l for l in listings if l.get('has_garden')])
            with_elevator = len([l for l in listings if l.get('has_elevator')])
            
            print(f"üí∞ Z cenami: {with_price}/{len(listings)} ({with_price/len(listings)*100:.1f}%)")
            print(f"üìç Z lokalizacjƒÖ: {with_location}/{len(listings)} ({with_location/len(listings)*100:.1f}%)")
            print(f"üìê Z powierzchniƒÖ: {with_area}/{len(listings)} ({with_area/len(listings)*100:.1f}%)")
            print(f"üö™ Z pokojami: {with_rooms}/{len(listings)} ({with_rooms/len(listings)*100:.1f}%)")
            print(f"üè¢ Z balkonem: {with_balcony}/{len(listings)} ({with_balcony/len(listings)*100:.1f}%)")
            print(f"üöó Z gara≈ºem: {with_garage}/{len(listings)} ({with_garage/len(listings)*100:.1f}%)")
            print(f"üåø Z ogrodem: {with_garden}/{len(listings)} ({with_garden/len(listings)*100:.1f}%)")
            print(f"üõó Z windƒÖ: {with_elevator}/{len(listings)} ({with_elevator/len(listings)*100:.1f}%)")
            
            # Statystyki ID i adres√≥w
            with_listing_id = len([l for l in listings if l.get('listing_id')])
            with_street = len([l for l in listings if l.get('street')])
            with_city = len([l for l in listings if l.get('city')])
            with_province = len([l for l in listings if l.get('province')])
            
            # Statystyki geocodingu
            with_coordinates = len([l for l in listings if l.get('latitude') and l.get('longitude')])
            
            print(f"üÜî Z ID og≈Çoszenia: {with_listing_id}/{len(listings)} ({with_listing_id/len(listings)*100:.1f}%)")
            print(f"üè† Z ulicƒÖ: {with_street}/{len(listings)} ({with_street/len(listings)*100:.1f}%)")
            print(f"üèôÔ∏è Z miastem: {with_city}/{len(listings)} ({with_city/len(listings)*100:.1f}%)")
            print(f"üìç Z wojew√≥dztwem: {with_province}/{len(listings)} ({with_province/len(listings)*100:.1f}%)")
            print(f"üåç Z wsp√≥≈Çrzƒôdnymi: {with_coordinates}/{len(listings)} ({with_coordinates/len(listings)*100:.1f}%)")
            
            print(f"\nüöÄ INFORMACJE O WIELOWƒÑTKOWO≈öCI:")
            print(f"   ‚Ä¢ U≈ºyto 4 wƒÖtk√≥w dla scrapowania szczeg√≥≈Ç√≥w")
            print(f"   ‚Ä¢ Znaczne przyspieszenie w por√≥wnaniu do wersji sekwencyjnej")
            print(f"   ‚Ä¢ Thread-safe error handling i progress tracking")
            print(f"   ‚Ä¢ Geocoding zintegrowany: {'‚úÖ W≈ÅƒÑCZONY' if with_coordinates > 0 else '‚ö†Ô∏è WY≈ÅƒÑCZONY'}")
            
            # Przyk≈Çad
            if listings:
                listing = listings[0]
                print(f"\nüìã PRZYK≈ÅAD NOWEJ STRUKTURY:")
                print(f"   ID: {listing.get('listing_id', 'brak')}")
                print(f"   Tytu≈Ç: {listing.get('title_raw', '')[:50]}...")
                print(f"   Cena: {listing.get('price', 0):,} z≈Ç")
                print(f"   Powierzchnia: {listing.get('area')} m¬≤")
                print(f"   Pokoje: {listing.get('rooms')}")
                print(f"   Adres surowy: {listing.get('address_raw', '')}")
                print(f"   Ulica: {listing.get('street', 'brak')}")
                print(f"   Dzielnica: {listing.get('district', 'brak')}")
                print(f"   Miasto: {listing.get('city', 'brak')}")
                print(f"   Wojew√≥dztwo: {listing.get('province', 'brak')}")
                print(f"   Rynek: {listing.get('market')}")
                print(f"   Balkon: {listing.get('has_balcony')}")
                print(f"   Gara≈º: {listing.get('has_garage')}")
                print(f"   Ogr√≥d: {listing.get('has_garden')}")
                print(f"   Winda: {listing.get('has_elevator')}")
                
                # Wsp√≥≈Çrzƒôdne
                lat = listing.get('latitude')
                lon = listing.get('longitude')
                if lat and lon:
                    print(f"   Wsp√≥≈Çrzƒôdne: {lat:.6f}, {lon:.6f} ‚úÖ")
                else:
                    print(f"   Wsp√≥≈Çrzƒôdne: Brak ‚ùå")
        else:
            print("‚ùå Nie pobrano ≈ºadnych og≈Çosze≈Ñ")
            
    except Exception as e:
        print(f"‚ùå B≈ÇƒÖd: {e}") 